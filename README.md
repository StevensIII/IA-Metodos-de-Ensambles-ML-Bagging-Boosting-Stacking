ğŸ“Š MÃ©todos de Ensamble en Machine Learning: Bagging, Boosting y Stacking



Este proyecto presenta la aplicaciÃ³n de modelos de ensamble en problemas de clasificaciÃ³n y regresiÃ³n, explorando tÃ©cnicas de Bagging, Boosting y Stacking. El objetivo es comparar su rendimiento, evaluar mÃ©tricas relevantes y demostrar cÃ³mo los ensambles mejoran la capacidad predictiva frente a modelos individuales.



ğŸ§‘â€ğŸ’» Autores



Stevens BohÃ³rquez







ğŸ“Œ Objetivos del Proyecto



Implementar modelos de ensamble en tareas de clasificaciÃ³n y regresiÃ³n.



Evaluar el desempeÃ±o mediante mÃ©tricas adecuadas para cada caso.



Aplicar mÃ©todos de bÃºsqueda de hiperparÃ¡metros (Grid Search y Random Search).



Comparar resultados y extraer conclusiones sobre la efectividad de Bagging, Boosting y Stacking.







ğŸ“Š DescripciÃ³n de los Problemas



ğŸ”¹ ClasificaciÃ³n



Dataset: tomado de UCI Machine Learning Repository

.



Objetivo: predecir la clase de un conjunto de instancias aplicando ensambles de Bagging, Boosting y Stacking.



MÃ©tricas evaluadas: PrecisiÃ³n, matriz de confusiÃ³n y otras medidas de desempeÃ±o.





ğŸ”¹ RegresiÃ³n



Dataset: tomado de UCI Machine Learning Repository

.



Objetivo: predecir una variable continua mediante ensambles de Bagging, Boosting y Stacking.



MÃ©tricas evaluadas: RMSE, RÂ² y error absoluto medio.





âš™ï¸ TecnologÃ­as Utilizadas



Python 3



Scikit-learn



Pandas, Numpy



Matplotlib, Seaborn





ğŸ“ˆ Resultados Destacados



Los mÃ©todos de Boosting (ej. Gradient Boosting) mostraron mejor rendimiento en clasificaciÃ³n.



En regresiÃ³n, el Stacking logrÃ³ un equilibrio entre sesgo y varianza, superando a modelos individuales.



La optimizaciÃ³n de hiperparÃ¡metros impactÃ³ significativamente en la mejora de las mÃ©tricas.





ğŸ“ Conclusiones



Los modelos de ensamble permiten mejorar la generalizaciÃ³n frente a algoritmos base.



No existe un Ãºnico mÃ©todo ganador; la elecciÃ³n depende del tipo de problema y del dataset.



Bagging es mÃ¡s robusto al sobreajuste, mientras que Boosting es mÃ¡s sensible pero con alto potencial predictivo.





ğŸ“Œ Referencias



UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/index.html



DocumentaciÃ³n de Scikit-learn

